---
title: "11- Creating Analysis Groups and Running Stats"
output:
  pdf_document: 
    fig_width: 20
    fig_height: 9
  html_document: 
    toc: true
    toc_float:
        collapsed: false
        smooth_scroll: true
    fig_width: 20
---

# Directory and doc rules

```{r, setup, eval=TRUE, include=TRUE, echo=FALSE}

# libraries
library(knitr) # output fotmatting
library(tidyr) # data wrangling
library(tidyverse) # data wrangling
library(dplyr) # data wrangling
library(vegan) # ecological stats 
library(cluster) # grouping metrics - VERIFY still needed
library(pgirmess) # stats - KW
library(ggplot2) # plots
library(factoextra) # pca/ nmds/ tweaking radars
library(FactoMineR) # pca/ nmds/ tweaking radars
library(FSA) # post hoc test - Dunn's Test       
library(rstatix) # VERIFY what this is for      
library(car) # VERIFY what this is for  
library(RVAideMemoire) # post hoc test for permanova
library(rcompanion) # KW testing
library(scales) # scaling data for IBR - works with data wrangling packages
library(fmsb) # polygon calculations for the radars

knitr::opts_chunk$set(
  root.dir = here::here(),
  echo = TRUE,         # show code chunks
  eval = TRUE,         # evaluate code chunks
  warning = FALSE,     # hide warnings
  message = FALSE,     # hide messages
  #fig.width = 15,       # set plot width in inches
  #fig.height = 9,      # set plot height in inches
  fig.align = "center" # slign plots to the center in output doc/ slide/ whatever
)

```

# Load & Check Data
```{r}

getwd()
#setwd("/Users/cmantegna/Documents/GitHub/WDFWmussels") # something here isn't working right - check out why

df1<- read.csv("../data/cleaned/ibr_1.csv") # penn cove reference site only
#df2<- read.csv("../data/cleaned/ibr_2.csv")
#df3<- read.csv("../data/cleaned/ibr_3.csv")


```

```{r}

# make categorical columns factors
df1$site_number <- as.factor(df1$site_number)
df1$sample_id <- as.factor(df1$sample_id)
df1$reporting_area <- as.factor(df1$reporting_area)

```

# Normality - Shapiro-Wilkes
```{r}
# fix names
shapiro.test(df1$p450_ibr1) # Not Normal
shapiro.test(df1$sod_ibr1) # Not Normal
shapiro.test(df1$shell_ibr1)
shapiro.test(df1$ci1_ibr1)
shapiro.test(df1$ci2_ibr1)
shapiro.test(df1$ci3_ibr1)


# checking the condition indices since they are throwing off the downstream process
summary(ibrv2i_df)
```

# Add groups to match WDFW reporting style
Groups
1. Biomarker- driven grouping - K-means clustering
2. Impervious Surface Percentage (0-10, 10-20, 20-40, 40-100)
3. Concentration Thresholds by contaminant - sketch this out to see if this is something I really need
4. Quantiles - same comment as above
```{r}

```

## Create biomarker- driven grouping
```{r}

# Use only rows with non-missing biomarker data
bio_clust_df <- ibrv2i_df %>%
  select(sample_id, p450, sod)

wss <- vector()

# Try values of k from 1 to 10
for (k in 1:10) {
  set.seed(42)
  wss[k] <- kmeans(bio_clust_df, centers = k, nstart = 25)$tot.withinss
}

# Plot the elbow
plot(1:10, wss, type = "b",
     pch = 19, frame = FALSE,
     xlab = "Number of clusters (k)",
     ylab = "Total within-cluster sum of squares",
     main = "Elbow Method for Choosing k")

set.seed(42)
k_opt <- 4  # elbow is distinctly at 2 with 3 close by, I have checked both with no real resolution

kmeans_result <- kmeans(bio_clust_df, centers = k_opt, nstart = 25)

# Add cluster assignment to your working df
bio_clust_df$bio_cluster4 <- as.factor(kmeans_result$cluster)

ibrv2i_df <- ibrv2i_df %>%
  left_join(bio_clust_df[, c("sample_id", "bio_cluster4")], by = "sample_id")

# saving ibrv2i df
#write.csv(ibrv2i_df, "/Users/cmantegna/Documents/GitHub/WDFWmussels/data/cleaned/ibrv2i_all_scaled_indices_groups.csv", row.names = FALSE)

```

## PCA plot to check clusters
```{r}

# Define variables for PCA
pca_vars <- c("p450", "sod", "shell", "ci2")

# Remove rows with any NA in the selected variables
pca_df <- ibrv2i_df %>%
  filter(if_all(all_of(pca_vars), ~ !is.na(.))) %>%
  select(sample_id, bio_cluster4, all_of(pca_vars))

# Keep only the scaled numeric matrix for PCA
pca_matrix <- pca_df %>%
  select(all_of(pca_vars)) %>%
  as.matrix()

pca_res <- prcomp(pca_matrix, center = TRUE, scale. = FALSE)  # already scaled relative to reference

# Create a scores dataframe
scores_df <- as.data.frame(pca_res$x)
scores_df$sample_id <- pca_df$sample_id
scores_df$bio_cluster4 <- pca_df$bio_cluster4

# Plot the first two PCs
ggplot(scores_df, aes(x = PC1, y = PC2, color = bio_cluster4)) +
  geom_point(size = 3, alpha = 0.8) +
  labs(title = "PCA of Scaled Biomarkers + Morphometrics",
       x = paste0("PC1 (", round(summary(pca_res)$importance[2, 1] * 100, 1), "% variance)"),
       y = paste0("PC2 (", round(summary(pca_res)$importance[2, 2] * 100, 1), "% variance)"),
       color = "Cluster") +
  theme_minimal() +
  theme(legend.position = "right")


```

## KW + Dunn Post Hoc, all but shell thickness as it is parametric
```{r}

library(dplyr)
library(FSA)

# Variables to test
variables <- c("p450", "sod", "shell", "ci2")

# Optional: store all significant Dunn's results in a list
all_dunn_results <- list()

for (var in variables) {
  cat("\n\n\n========================================\n")
  cat("### Testing:", var, "\n")
  cat("========================================\n")
  
  # Subset and clean data
  df <- ibrv2i_df %>%
    select(all_of(c(var, "site_name", "reporting_area"))) %>%
    filter(!is.na(.data[[var]]))
  
  y <- df[[var]]

  ## --- Kruskal-Wallis by site_name ---
  cat("\n--- Kruskal-Wallis for", var, "by site_name ---\n")
  kw_site <- kruskal.test(y ~ site_name, data = df)
  print(kw_site)

  ## --- Dunn's post hoc by site_name ---
  if (kw_site$p.value < 0.05) {
    cat("\n>>> Dunn's Test (BH-adjusted) for", var, "by site_name <<<\n")
    dunn_site <- dunnTest(y ~ site_name, data = df, method = "bh")
    sig_site <- dunn_site$res %>% filter(P.adj < 0.05)
    print(sig_site)

    # Store if needed
    all_dunn_results[[paste0(var, "_site")]] <- sig_site
  } else {
    cat("No significant differences by site_name (KW p >= 0.05)\n")
  }

  ## --- Kruskal-Wallis by reporting_area ---
  cat("\n--- Kruskal-Wallis for", var, "by reporting_area ---\n")
  kw_area <- kruskal.test(y ~ reporting_area, data = df)
  print(kw_area)

  ## --- Dunn's post hoc by reporting_area ---
  if (kw_area$p.value < 0.05) {
    cat("\n>>> Dunn's Test (BH-adjusted) for", var, "by reporting_area <<<\n")
    dunn_area <- dunnTest(y ~ reporting_area, data = df, method = "bh")
    sig_area <- dunn_area$res %>% filter(P.adj < 0.05)
    print(sig_area)

    # Store if needed
    all_dunn_results[[paste0(var, "_reporting")]] <- sig_area
  } else {
    cat("No significant differences by reporting_area (KW p >= 0.05)\n")
  }
}

# Save each significant Dunn's test result to file
for (name in names(all_dunn_results)) {
  write.csv(all_dunn_results[[name]], file = paste0("dunn_results_", name, ".csv"), row.names = FALSE)
}


```

## ANOVA for shell
```{r}

library(emmeans)
library(multcomp)
library(multcompView)

# Prepare cleaned dataframe
df <- ibrv2i_df %>%
  select(shell, site_name, reporting_area) %>%
  filter(!is.na(shell))

# Create a list to store results
tukey_group_letters <- list()

# ---------- ANOVA by site_name ----------
cat("=== ANOVA: shell ~ site_name ===\n")
aov_site <- aov(shell ~ site_name, data = df)
summary_site <- summary(aov_site)
print(summary_site)

# Extract p-value safely
p_site <- summary_site[[1]][["Pr(>F)"]][1]

if (!is.na(p_site) && p_site < 0.05) {
  cat("\n>>> Tukey's HSD (site_name) <<<\n")
  tukey_site <- TukeyHSD(aov_site)
  print(tukey_site$site_name)

  # Get group letters
  em_site <- emmeans(aov_site, pairwise ~ site_name)
  cld_site <- multcomp::cld(em_site$emmeans, Letters = letters, sort = FALSE)

  # Clean table
  cld_site_out <- cld_site %>%
    select(site_name, emmean, SE, df, lower.CL, upper.CL, .group) %>%
    rename(group = .group)

  print(cld_site_out)

  # Store in list
  tukey_group_letters$site_name <- cld_site_out
} else {
  cat("No significant differences among sites.\n")
}


# ---------- ANOVA by reporting_area ----------
cat("\n\n=== ANOVA: shell ~ reporting_area ===\n")
aov_area <- aov(shell ~ reporting_area, data = df)
summary_area <- summary(aov_area)
print(summary_area)

# Extract p-value safely
p_area <- summary_area[[1]][["Pr(>F)"]][1]

if (!is.na(p_area) && p_area < 0.05) {
  cat("\n>>> Tukey's HSD (reporting_area) <<<\n")
  tukey_area <- TukeyHSD(aov_area)
  print(tukey_area$reporting_area)

  # Get group letters
  em_area <- emmeans(aov_area, pairwise ~ reporting_area)
  cld_area <- multcomp::cld(em_area$emmeans, Letters = letters, sort = FALSE)

  # Clean table
  cld_area_out <- cld_area %>%
    select(reporting_area, emmean, SE, df, lower.CL, upper.CL, .group) %>%
    rename(group = .group)

  print(cld_area_out)

  # Store in list
  tukey_group_letters$reporting_area <- cld_area_out
} else {
  cat("No significant differences among reporting areas.\n")
}

write.csv(tukey_group_letters$site_name, "/Users/cmantegna/Documents/GitHub/WDFWmussels/output/tukey_shell_site_name.csv", row.names = FALSE)
write.csv(tukey_group_letters$reporting_area, "/Users/cmantegna/Documents/GitHub/WDFWmussels/output/tukey_shell_reporting_area.csv", row.names = FALSE)



```



# Run PCAs/ NMDSs/ Permanova/ Correlations + Heatmaps/ Kendall's Tau
# Visualization
## NWS Poster viz
```{r}

#map
library(sf)
library(readr)
library(patchwork)
library(viridis)
library(rnaturalearth)
library(ggsignif)

df$reporting_area <- as.factor(df$reporting_area)

# Convert to spatial
sites_sf <- st_as_sf(df, coords = c("longitude", "latitude"), crs = 4326)

coast <- ne_states(country = "united states of america", returnclass = "sf") %>%
  filter(name == "Washington")

# Major city coordinates for labels
cities <- data.frame(
  city = c("Seattle", "Tacoma", "Olympia", "Bellingham", "Port Angeles", "Hama Hama"),
  lon = c(-122.33, -122.45, -122.90, -122.48, -123.43, -123.16),
  lat = c(47.61, 47.25, 47.00, 48.73, 48.08, 47.63)
)

# Define color palette
area_colors <- viridis::viridis(length(levels(df$reporting_area)), option = "turbo")

# Map
map_plot <- ggplot() +
  geom_sf(data = coast, fill = "gray90", color = "white") +
  geom_sf(data = sites_sf, aes(color = reporting_area), size = 3, alpha = 0.9) +
  geom_text(data = cities, aes(x = lon, y = lat, label = city),
            color = "black", fontface = "bold", size = 4) +
  scale_color_manual(values = area_colors, name = "Reporting Area") + 
  coord_sf(xlim = c(-124, -121), ylim = c(46.5, 49.0)) +
  theme_void() +
  labs(title = "Sampled Sites by Reporting Area")

# Filter data for IBRv2i < 5
df_filtered <- df %>% filter(IBRv2i < 5)
df_filtered <- df_filtered %>%
  mutate(is_reference = ifelse(site_name == "Penn Cove Reference", "Reference", "Other"))

# Boxplot
box_plot <- ggplot(df_filtered, aes(x = reporting_area, y = IBRv2i, fill = reporting_area)) +
  geom_boxplot(alpha = 0.8, outlier.shape = NA) +
  geom_jitter(width = 0.2, color = "black", size = 1.5) +
  scale_fill_manual(values = area_colors, guide = "none") + 
  coord_cartesian(ylim = c(-3, 5.5)) +
  geom_signif(comparisons = list(
      c("11", "7"),
      c("13", "7"),
      c("6", "7"),
      c("10", "13")
    ),
    annotations = c("p = 0.0093", "p = 0.0013", "p = 0.0487", "p = 0.0199"),
    y_position = c(5.2, 4.8, 4.5, 4.2),
    tip_length = 0.02,
    textsize = 4.5
  ) +
  theme_minimal(base_size = 13) + 
  theme(
    axis.line = element_line(size = 1.2, color = "black"),
    axis.text = element_text(size= 25, face = "bold"),
    axis.title = element_text(size= 30, face = "bold"),
    plot.title = element_text(hjust = 0.5, face = "bold")
  ) +
  labs(x = "Reporting Area", y = "IBRv2i Score")
  
# Combine
combo_plot <- map_plot + box_plot +
  plot_layout(ncol = 2, widths = c(1.3, 1)) +
  plot_annotation(title = "Integrated Biomarker Response Overview")

# Display
print(map_plot)
print(box_plot)

# Export 
ggsave("/Users/cmantegna/Documents/GitHub/WDFWmussels/output/poster_map.png", map_plot, width = 14, height = 8, dpi = 300)

ggsave("/Users/cmantegna/Documents/GitHub/WDFWmussels/output/poster_box.png", box_plot, width = 14, height = 8, dpi = 300)


```
## KW for plotting
```{r}

# Make sure reporting_area is a factor
df$reporting_area <- as.factor(df$reporting_area)

# Run Kruskal-Wallis test
kruskal_result <- kruskal.test(IBRv2i ~ reporting_area, data = df)
print(kruskal_result)

# Run Dunn's post hoc test
dunn_result <- dunnTest(IBRv2i ~ reporting_area, data = df, method = "holm")
print(dunn_result)


```



PCA/ NMDS
Correlation heatmaps
```{r}

```

