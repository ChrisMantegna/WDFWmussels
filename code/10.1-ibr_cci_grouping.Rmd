---
title: "10.1- Creating Analysis Groups and Running Stats"
output:
  pdf_document: 
    fig_width: 20
    fig_height: 9
  html_document: 
    toc: true
    toc_float:
        collapsed: false
        smooth_scroll: true
    fig_width: 20
---

# Directory and doc rules

```{r, setup, eval=TRUE, include=TRUE, echo=FALSE}

# libraries
library(knitr) # output formatting
library(tidyr) # data wrangling
library(tidyverse) # data wrangling
library(dplyr) # data wrangling
library(vegan) # ecological stats 
library(cluster) # grouping metrics - VERIFY still needed
library(pgirmess) # stats - KW
library(ggplot2) # plots
library(factoextra) # pca/ nmds/ tweaking radars
library(FactoMineR) # pca/ nmds/ tweaking radars
library(FSA) # post hoc test - Dunn's Test       
library(rstatix) # VERIFY what this is for      
library(car) # VERIFY what this is for  
library(RVAideMemoire) # post hoc test for permanova
library(rcompanion) # for annotation of permanova
library(scales) # scaling data for IBR - works with data wrangling packages
library(fmsb) # polygon calculations for the radars
library(devtools)

knitr::opts_chunk$set(
  root.dir = here::here(),
  echo = TRUE,         # show code chunks
  eval = TRUE,         # evaluate code chunks
  warning = FALSE,     # hide warnings
  message = FALSE,     # hide messages
  #fig.width = 15,       # set plot width in inches
  #fig.height = 9,      # set plot height in inches
  fig.align = "center" # slign plots to the center in output doc/ slide/ whatever
)

```

# Load & Check Data
```{r}

getwd()
#setwd("/Users/cmantegna/Documents/GitHub/WDFWmussels") # something here isn't working right - check out why

all_data<- read.csv("../data/index_df/ibr_for_averaging.csv")
avg_data<- read.csv("../data/index_df/avg_metrics_ibr_indices_for_correlations.csv")   

```

```{r}

# make reporting area a factor
all_data$reporting_area <- as.factor(all_data$reporting_area)
avg_data$reporting_area <- as.factor(avg_data$reporting_area)


```

# Normality - Shapiro-Wilkes
```{r}
# all data - metrics & ibrs
shapiro.test(all_data$log2_p450) # Normal
shapiro.test(all_data$log2_sod) # Not Normal
shapiro.test(all_data$log2_shell) # Not Normal
shapiro.test(all_data$log2_ci1) # Not Normal
shapiro.test(all_data$log2_ci2) # Not Normal
shapiro.test(all_data$log2_ci3) # Not Normal
shapiro.test(all_data$ibr_biomarker) # Not Normal
shapiro.test(all_data$ibr_morphometric) # Not Normal
shapiro.test(all_data$ibr_combined) # Not Normal

# avg data - metrics
shapiro.test(avg_data$p450) # Normal
shapiro.test(avg_data$sod) # Not Normal
shapiro.test(avg_data$shell) # Not Normal
shapiro.test(avg_data$ci1) # Not Normal
shapiro.test(avg_data$ci2) # Normal
shapiro.test(avg_data$ci3) # Normal
shapiro.test(avg_data$weight_initial) # Normal
shapiro.test(avg_data$weight_change) # Normal
shapiro.test(avg_data$weight_final) # Normal
shapiro.test(avg_data$height) # Normal
shapiro.test(avg_data$length) # Normal
shapiro.test(avg_data$width) # Normal

#indices
shapiro.test(avg_data$ibr_bio) # Not Normal
shapiro.test(avg_data$ibr_morph) # Normal
shapiro.test(avg_data$ibr_combined) # Normal
shapiro.test(avg_data$chlordane_index) # Not Normal
shapiro.test(avg_data$ddt_index) # Not Normal
shapiro.test(avg_data$hch_index) # Not Normal
shapiro.test(avg_data$heavy_metal_index) # Not Normal
shapiro.test(avg_data$nutrient_metal_index) # Not Normal
shapiro.test(avg_data$total_metal_index) # Not Normal
shapiro.test(avg_data$pah_lmw_index) # Not Normal
shapiro.test(avg_data$pah_hmw_index) # Not Normal
shapiro.test(avg_data$pah_add_index) # Not Normal
shapiro.test(avg_data$pbde_index) # Not Normal
shapiro.test(avg_data$pcb_index) # Not Normal
shapiro.test(avg_data$pesticide_index) # Not Normal

```

## Create biomarker- driven grouping
### This code is incorrect - fix it before re-running
```{r}

# ibr overall yields the same result as the kmeans below - no need to rerun
# Use only rows with non-missing biomarker data
bio_clust_df <- all_data %>%
  select(log2_p450, log2_sod, ibr_combined)

wss <- vector()

# Try values of k from 1 to 10
for (k in 1:10) {
  set.seed(42)
  wss[k] <- kmeans(bio_clust_df, centers = k, nstart = 25)$tot.withinss
}

# Plot the elbow
plot(1:10, wss, type = "b",
     pch = 19, frame = FALSE,
     xlab = "Number of clusters (k)",
     ylab = "Total within-cluster sum of squares",
     main = "Elbow Method for Choosing k")

set.seed(42)
k_opt <- 3  # elbow is distinctly at 3, I have checked both with no real resolution

kmeans_result <- kmeans(bio_clust_df, centers = k_opt, nstart = 25)

# Add cluster assignment to your working df
bio_clust_df$cluster3 <- as.factor(kmeans_result$cluster)

data <- data %>%
  left_join(bio_clust_df[, c("sa", "cluster3")], by = "sample_id")

# saving ibrv2i df
#write.csv(ibrv2i_df, "/Users/cmantegna/Documents/GitHub/WDFWmussels/data/cleaned/ibrv2i_all_scaled_indices_groups.csv", row.names = FALSE)

```

## PCA plot to check clusters
### This code is incorrect - fix it before re-running
```{r}

# Define variables for PCA
pca_vars <- c("p450", "sod", "shell", "ci1")

# Remove rows with any NA in the selected variables
pca_df <- data %>%
  filter(if_all(all_of(pca_vars), ~ !is.na(.))) %>%
  select(sample_id, geo_cluster_2, all_of(pca_vars))

# Keep only the scaled numeric matrix for PCA
pca_matrix <- pca_df %>%
  select(all_of(pca_vars)) %>%
  as.matrix()

pca_res <- prcomp(pca_matrix, center = TRUE, scale. = FALSE)  # already scaled relative to reference

# Create a scores dataframe
scores_df <- as.data.frame(pca_res$x)
scores_df$sample_id <- pca_df$sample_id
scores_df$geo_cluster_2 <- pca_df$geo_cluster_2

# Plot the first two PCs
ggplot(scores_df, aes(x = PC1, y = PC2, color = geo_cluster_2)) +
  geom_point(size = 3, alpha = 0.8) +
  labs(title = "PCA of Scaled Biomarkers + Morphometrics",
       x = paste0("PC1 (", round(summary(pca_res)$importance[2, 1] * 100, 1), "% variance)"),
       y = paste0("PC2 (", round(summary(pca_res)$importance[2, 2] * 100, 1), "% variance)"),
       color = "Cluster") +
  theme_minimal() +
  theme(legend.position = "right")

```

## Create geographic grouping - review with avg table, all sample table looks good - 5/8/25
### K-Means for Geographic Groups

```{r}

## 2,3, and 6 clusters are optimal using this method - test with anosim

geo_data <- data.frame(lat = all_data$latitude, lon = all_data$longitude) # pulling out lat/ long

fviz_nbclust(geo_data, kmeans, method = "wss") +
  ggtitle("Elbow Method for Optimal Geographic Clusters") # Compute within-cluster sum of squares for different k values

fviz_nbclust(geo_data, kmeans, method = "silhouette") +
  ggtitle("Silhouette Method for Geographic Clustering") # visual check cluster quality

set.seed(123)  # Ensure reproducibility

gap_stat <- clusGap(geo_data, FUN = kmeans, K.max = 10, B = 50)
fviz_gap_stat(gap_stat) # statistical test for cluster quality

# plot to see that the optimal cluster count is a reliable metric in real life
set.seed(123)  # Ensure consistent results
k_optimal <- 9  # Choose based on elbow/silhouette/gap methods

# Run k-means
km <- kmeans(geo_data, centers = k_optimal, nstart = 25)

# Add cluster assignments to dataset
all_data$cluster9 <- as.factor(km$cluster)

ggplot(all_data, aes(x = longitude, y = latitude, color = cluster9)) +
  geom_point(size = 3) +
  ggtitle("Geographic Clusters of Sampling Sites") +
  xlab("Longitude") + ylab("Latitude")


```

# ANOSIM, Analysis of similarity
## no geographic groups are statistically robust enough for further analysis
```{r}

df_anosim <- all_data

# Create a new distance matrix
dist_matrix <- vegdist(df_anosim %>%
                         select(log2_p450, log2_sod, log2_shell, ibr_combined) %>%
                         scale(), method = "euclidean")

# Run ANOSIM
anosim_results <- anosim(dist_matrix, grouping = df_anosim$cluster9)
print(anosim_results)

#Interpretation:
#If R > 0.5 and p < 0.05, group differences are meaningful.
#If R < 0.2, groups are not well-separated.

```

# Analysis of Variance & Dispersion
## Variance, p-value= 4.866e-05. We will proceed with KW testing - 4.23.25
```{r}

# explanation is code file: 03-creating_analysis_dfs

# all data
all_data$site_name <- as.factor(all_data$site_name) # change site names to a factor instead of a character
metric_matrix <- as.matrix(all_data[, 7:20]) # pull out my metrics - confirm column numbers are still correct
dist_matrix_variance <- vegdist(metric_matrix, na.rm= TRUE, method = "bray") # create a distance matrix
dispersion_test <- betadisper(dist_matrix_variance, group = all_data$site_name) # check homogeneity of group variances
anova(dispersion_test) # Perform permutation test for homogeneity. A p-value > .05 means we need to use PERMANOVA. 
boxplot(dispersion_test) # visualize results (boxplot of dispersions)

# avg data
avg_data$site_name <- as.factor(avg_data$site_name) # change site names to a factor instead of a character
metric_matrix <- as.matrix(avg_data[, ]) # pull out my metrics - confirm column numbers are still correct
dist_matrix_variance <- vegdist(metric_matrix, na.rm= TRUE, method = "bray") # create a distance matrix
dispersion_test <- betadisper(dist_matrix_variance, group = avg_data$site_name) # check homogeneity of group variances
anova(dispersion_test) # Perform permutation test for homogeneity. A p-value > .05 means we need to use PERMANOVA. 
boxplot(dispersion_test) # visualize results (boxplot of dispersions)

```


